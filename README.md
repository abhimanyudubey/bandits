# Multi-Armed Bandits

The relevant library is `bandits` and some basic tests are present in `tests`.

Currently, we've implemented `UCB1` and random agents for the Bernoulli bandit environment.
